{"cells":[{"cell_type":"markdown","source":"### XDA Scraper (with Notes)","metadata":{"tags":[],"cell_id":"b78bb735338e4487b542a9786849a0c5","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# XDA Scraper without notes\n\n# Scrape S21 posts from XDA-Developers website's S21 page\n\n    # for new models,\n        # check whether it was posted within 1 day\n        # get post url & comment number\n        # go to next page and check again\n        # go to each url and get title, contents, # comment\n\n\nimport urllib.request\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\nimport ssl\nimport time\nimport datetime\n\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\n\n\n\ntaglist = []        # title\nlinklist = []       # url\nlastlist = []       # last activity time\ndatelist = []       # post creation time\nlabellist = []      # label (Y/N)\ncomlist = []        # number of comments\nmodellist = []      # model\n\n\n###\n# For long weekends, simply add the name(s) of day that you missed(ex2), or day up to 2 days ago(ex1) to the timewords below\n# ex1. off from Fri to Tue, doing VOC on Wed --> add \"Sunday\" & \"Monday\" \n# ex2. off from Wed to Sun, doing VOC on Mon --> add \"Wed\" & \"Thur\" \n###\nvocday = input(\"Is it Monday or nah? (Enter y/n):\")\nif vocday == \"y\":\n    print(\"Gotcha fam. Take it easy.\")\n    timewords = ['now', 'second ago', 'A moment ago', '1 minute ago', 'minutes ago', 'hour ago', 'hours ago', 'Today', 'Yesterday', 'Saturday', 'Friday']\nelse:\n    print(\"Gotcha fam. One day closer to Friday, amirite?\")\n    timewords = ['now', 'second ago', 'A moment ago', '1 minute ago', 'minutes ago', 'hour ago', 'hours ago', 'Today', 'Yesterday']\n\n\ntime.sleep(2.5)\n\n\n## # How to get today's date & time\n## # https://www.programiz.com/python-programming/datetime/current-datetime\n##from datetime import date\n##print(\"Today is \", date.today())\n##print(\"Right now is\", datetime.now())\n##dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n##print(\"date and time =\", dt_string)\n##\n## # https://www.hellocodeclub.com/python-get-day-of-week/\n##import datetime      \n##week_days=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n##week_num=datetime.date(2021,8,5).weekday()\n##print(week_days[week_num])\n\n\n\n# https://stackoverflow.com/questions/16627227/problem-http-error-403-in-python-3-web-scraping\n# urllib.error.HTTPError: HTTP Error 403: Forbidden\n# Need a different way to get around the urllib web scraper block\n\nclass AppURLopener(urllib.request.FancyURLopener):\n    version = \"Mozilla/5.0\"\nopener = AppURLopener()\n#response = opener.open('http://httpbin.org/user-agent')\n\n# define a function for retrieving titles & post links on a results page\ndef scraperesults(searchpage, timewords):\n    \n    print(\"Scraping a new model/page...\")\n    # url = searchpage\n    response = opener.open(searchpage)\n    #html = urlopen(url, context=ctx).read()\n    soup = BeautifulSoup(response, \"html.parser\")\n\n    # Determine the latest activity on the posts\n    threads = soup.find_all(\"div\", class_=\n            re.compile(r\"structItem[ \\t]+structItem--thread[ \\t]+is-prefix[0-9]+[ \\t]+js-inlineModContainer[ \\t]+js-threadListItem-[0-9]+\"))   # find all threads. Remove ', text=True' ! \n    \n    for thread in threads:\n        lastacttime = thread.find(\"time\", class_=re.compile(r'structItem-latestDate[ \\t]+u-dt')).text\n        #print(lastacttime)\n\n        # https://stackoverflow.com/questions/3271478/check-list-of-words-in-another-string\n        if any(word in lastacttime for word in timewords):                      # if any of the keywords is found as latest comment time,\n            taglist.append(thread.find_all('a', text=True)[1].text)             # scrape title\n            #print(thread.find_all('a', text=True)[1].text)     # find 2nd 'a' element under post\n            \n            linklist.append(\"https://forum.xda-developers.com\" + thread.find_all('a', text=True)[1]['href'])    # scrape hyperlink\n            # print(thread.find_all('a', text=True)[1]['href'])\n            # print(thread.find_all('a', text=True)[0]['href']) # this also works\n            \n            lastlist.append(lastacttime)                                        # scrape last activity time\n            # print(lastacttime)\n\n            datelist.append(thread.find_all('a', text=True)[3].text)            # scrape post creation time\n            #print(thread.find_all('a', text=True)[3].text)\n\n            comlist.append(int(''.join(re.findall('[0-9]+', str(thread.find('dd'))))))       # scrape number of comments\n            #print(int(''.join(re.findall('[0-9]+', str(thread.find('dd'))))))       # re.findall(blah) gives ['5']. use ''.join(blah) to unlist the number as string\n\n            # can't tell model by looking at final URL; strip model name from the front-page-of-the-board URL\n            #print(searchpage.lstrip(\"https://forum.xda-developers.com/f/samsung-galaxy-\").rstrip(r\".[0-9]+/|.[0-9]+/page-[0-9]\").replace(\"-\",\" \"))\n            #input()\n            modellist.append(searchpage.lstrip(\"https://forum.xda-developers.com/f/samsung-galaxy-\").rstrip(r\".[0-9]+/|.[0-9]+/page-[0-9]\").replace(\"-\",\" \"))\n            \n\n\n    print(\"-----------Going to next page...----------\")\n    time.sleep(0.5)   # add 0.5s delay\n\n\n\n# for new, major S-line models, do 1-3 pages\nsearchpages = [\"https://forum.xda-developers.com/f/samsung-galaxy-s22-ultra.12515/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-s22-ultra.12515/page-2\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-s22-ultra.12515/page-3\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-tab-s8.12521/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-fold-4.12657/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-fold-4.12657/page-2\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-flip-4.12655/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-flip-4.12655/page-2\",\n               # \"https://forum.xda-developers.com/f/samsung-galaxy-s21-fe.12389/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-s21.11937/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-fold3.12349/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-flip-3.12351/\"\n               ]\n\n\n# scrape page 1~4 of S21 and page 1~2 of Fold, ZFlip & S20(FE) for post links\n\nfor searchpage in searchpages:\n    scraperesults(searchpage, timewords)\n\n\nprint(\"\\nNumber of taglist: \", len(taglist), \"posts\",\n      \"\\nNumber of linklist: \", len(linklist), \"posts\")\n\n# input()\n\n\n\n\n\n\nbodylist = []\n\n\n# Open up all links and get data (Crawl)\n\nfor hyperlink in linklist:\n    # url = hyperlink\n    response = opener.open(hyperlink)\n    # html = urlopen(url, context=ctx).read()\n    soup = BeautifulSoup(response, \"html.parser\")\n\n    print(\"Scraping...\")\n\n    try:                # if there's any error such as \"ValueError: read of closed file\" skip over the error with try/except. Press enter in prompt to continue\n        # Body text\n        # body = soup.find(\"article\", class_=\"message-body[ \\t]+js-selectToQuote\") # .find(\"br\", text=True)\n        body = soup.find(\"div\", class_=\"bbWrapper\")\n        # if body.find(\"p\") != None:\n        #       print(body.find_all(\"p\"))\n        bodylist.append(body.text.strip())   # remove whitespace at beginning & end\n        # print(body.text.strip())\n    except:\n        pass\n\n\n    time.sleep(0.5)  # add 0.5s delay\n\n\n\n\n    # instead of detecting multiple pages on post and navigating to last page + last comment,\n    # on results page, detect # of comments and latest comment date\n    # follow latest commenter's profile, and look at date of \"Re: \" + title\n\n    # Last comment date\n\n\n    # ========= forget the comment count method below, and focus on detecting latest comment =======\n##    \n##    if comm >= 1 && comm < 10:        # page 2 forms at 10 comments\n##        # fetch the comment dates\n##        lastdatec = soup.find_all('span', class_='DateTime lia-message-posted-on lia-component-common-widget-date')     # not list. It's bs4.element.ResultSet, each entry bs4.element.Tag\n##        lastdate = datetime.fromtimestamp(0)      # create a datetime object to compare last post time to\n##\n##        i=0\n##        \n##        for date in lastdatec:      # find the latest date\n##            i += 1\n##            print(i)\n##            if i == len(lastdatec):\n##                # for old posts, class_=\"local-date\" & class_=\"local-time\" are shown, with times written under .text\n##                if date.find(\"span\", class_=\"local-date\"):\n##                    date = date.find(\"span\", class_=\"local-date\").text + \" \" + date.find(\"span\", class_=\"local-time\").text\n##                    date = datetime.strptime(date.strip('\\u200e'), '%m-%d-%Y %I:%M %p')\n##\n##                # for newer posts, class_=\"local-friendly-date\" is shown, with time written inside title=\"08-01-2021 09:26 AM\"   \n##                else:      # date.find(\"span\")['title'] exists instead\n##                    date = date.find(\"span\")['title']\n##                    date = datetime.strptime(date, '\\u200e%m-%d-%Y %I:%M %p')   # or (startdate.strip('\\u200e'), '%m-%d-%Y %I:%M %p')\n##                print(date)     # class 'datetime.datetime'\n##                \n##                if date > lastdate:\n##                    lastdate = date\n##        i += 1\n##\n##        lastlist.append(lastdate)\n##    else:\n##        lastdate = '-'\n##        lastlist.append(lastdate)\n\n##   lolol = input()\n\n\n\n\n\n# Create DataFrame and export to Excel file\n\nimport pandas as pd\n\n# print(len(taglist), len(linklist), len(datelist), len(bodylist), len(comlist))\n\ndictio = {'Model' : modellist,\n          'Priority' : [\"C\"]*len(taglist),  # create a list of C's\n          'Title' : taglist,\n          'Body Text' : bodylist,\n          'Thread' : [1]*len(taglist),   # create a list of 1's\n          'Comments' : comlist,\n          'URLs' : linklist,\n          'Start Date' : datelist,\n          'Last Date' : lastlist,\n          'Issue?' : ['Y']*len(taglist)   # for now, make them all Y\n          }\ndf = pd.DataFrame(dictio)\nprint(df)\n\ninput()\n\n\n# Post-processing DataFrame\n# For models other than S21, if 0 comment, drop the data\n\n\n# https://stackoverflow.com/questions/6531482/how-to-check-if-a-string-contains-an-element-from-a-list-in-python\n# https://stackoverflow.com/questions/28876243/how-to-delete-the-current-row-in-pandas-dataframe-during-df-iterrows\n\n# if any portion of the hyperlink matches any of those in zerocommentpages AND there's 0 comment, drop that row\n#for indexx, roww in df.iterrows():\n#    if roww['Comments'] == 0:\n#        if any(page in roww['URLs'] for page in zerocommentpages):\n#            df.drop(indexx, inplace=True)\n\n\"\"\"\nfor indexx, roww in df.iterrows():\n    if roww['Comments'] == 0:\n        if \"us.community.samsung.com/t5/Galaxy-Fold/\" in roww['URLs']:\n            df.drop(indexx, inplace=True)\n        elif \"us.community.samsung.com/t5/Galaxy-Z-Flip/\" in roww['URLs']:\n            df.drop(indexx, inplace=True)\n        elif \"us.community.samsung.com/t5/Galaxy-S20/\" in roww['URLs']:\n            df.drop(indexx, inplace=True)\nprint(df)\n\"\"\"\n\n\n#df2 = pd.read_excel(\"samsungscrap.xlsx\")\n\n#df.append(df2) or concatenate\n\ntodaystring = datetime.datetime.now().strftime(\"%Y%m%d\")    # 20220331\nfilenom = \"xdascrap\"\nfileext = \".xlsx\"\nfilename = str(filenom + todaystring + fileext)\n\ndf.to_excel(filename, sheet_name=str(datetime.date.today()))     # Create a new file (can overwrite, so careful)\n# df.to_csv('samsungscrap.csv')\n\n\n\n\n\n# https://stackoverflow.com/questions/47737220/append-dataframe-to-excel-with-pandas\n\n# 2021. 8. 18 Start from here! \n# https://www.guru99.com/python-csv.html\n# https://realpython.com/python-csv/\n\n\n# using import csv, read table from previous file\n# find & replace duplicates, update file\n    # for N & R, drop the new duplicate\n    # for Y, drop the old duplicate\n\n\n\n# SQL database update\n\n\n\n\n\n\n\nlol = input()\n\n\n\n\n\n\n\n# for body text,\n# body = soup.find_all(\"p\", text=True)\n# bodylist.append(body[0].get_text())\n# doesn't account for multiple p's (several paragraphs)\n# Also, sometimes there is no p at all - just (\"div\", class_=\"lia-message-body-content\")\n\n\n### Scrape 5 posts from Samsung Community website\n##\n##    # scrape a page\n##    # scrape from a results page\n##    # scrape posts based on date\n##    # write to CSV\n##    # save to database\n##\n##\n##\n### scrape a particular page\n##url = \"https://us.community.samsung.com/t5/Galaxy-Z-Flip/Whatsapp-s-Picture-in-Picture-Video-Call-Screen-Does-Not-Show/m-p/1843314\"\n##html = urlopen(url, context=ctx).read()\n##soup = BeautifulSoup(html, \"html.parser\")\n##\n##\n### Retrieve all of the h1 tags\n##tags = soup('h1', '_eYtD2XCVieq6emjKBH3m')\n##print(len(tags))    # why are there 2, not 1?\n##for tag in tags:\n##    # Look at the parts of a tag\n##    # print('TAG:', tag)\n##    # print('URL:', tag.get('href', None))\n##    print('Title:', tag.contents[0])\n##    # print('Attrs:', tag.attrs)\n##\n##\n##tags1 = soup('p', '_1qeIAgB0cPwnLhDF9XSiJM')    # length 5: body & comments\n##tags1 = tags1[0]\n##print('Body:', tags1.contents[0])\n##\n##\n##tags2 = soup('span', 'FHCV02u6Cp2zYL0fhQPsO')   # length 1\n##print('Number of Comments:', tags2[0].contents[0].strip(\" coments\"))\n### From \"3 comments\", remove whitespace, c, o, m, e, n, t, s from both ends of string\n##\n##\n##tags3 = soup('a', '_3jOxDPIQ0KaOWpzvSQo-1s')\n##print('Hyperlink:', tags3[0].get('href', None))\n##\n##\n##\n##\n### results page scrape for zflip\n### https://us.community.samsung.com/t5/Galaxy-Z-Flip/bd-p/get-help-galaxy-ZFlip\n##\n\n\n\n\n### Attempt at figuring out last comment date by going to last page + last comment\n##    templist = []\n##    if soup.find('ul', class_\"lia-paging-full-pages\"):   # multiple pages of comments\n##        pages = soup.find('ul', class_\"lia-paging-full-pages\")\n##        pagelen = len(pages)\n##        nexturl = pages.find('li', class_='lia-paging-page-last lia-js-data-pageNum-' + pagelen)['href']\n##        templist.append(\n##    \n##    elif comm >= 1 && comm < 10:        # page 2 forms at 10 comments\n##        # fetch the comment dates\n##        lastdatec = soup.find_all('span', class_='DateTime lia-message-posted-on lia-component-common-widget-date')     # not list. It's bs4.element.ResultSet, each entry bs4.element.Tag\n##        lastdate = datetime.fromtimestamp(0)      # create a datetime object to compare last post time to\n##        print(lastdate)\n##        print(datetime.fromtimestamp(1))\n##        i=0\n##        print(len(lastdatec))\n##        for date in lastdatec:      # find the latest date\n##\n##            # for old posts, class_=\"local-date\" & class_=\"local-time\" are shown, with times written under .text\n##            if date.find(\"span\", class_=\"local-date\"):\n##                date = date.find(\"span\", class_=\"local-date\").text + \" \" + date.find(\"span\", class_=\"local-time\").text\n##                date = datetime.strptime(date.strip('\\u200e'), '%m-%d-%Y %I:%M %p')\n##\n##            # for newer posts, class_=\"local-friendly-date\" is shown, with time written inside title=\"08-01-2021 09:26 AM\"   \n##            else:      # date.find(\"span\")['title'] exists instead\n##                date = date.find(\"span\")['title']\n##                date = datetime.strptime(date, '\\u200e%m-%d-%Y %I:%M %p')   # or (startdate.strip('\\u200e'), '%m-%d-%Y %I:%M %p')\n##            print(date)     # class 'datetime.datetime'\n##            \n##            if date > lastdate:\n##                lastdate = date\n##\n##        lastlist.append(lastdate)\n##    else:\n##        lastdate = '-'\n##        lastlist.append(lastdate)\n\n\n\n### go to filtered search results page (shows less S21 issues)\n##url = \"https://us.community.samsung.com/t5/forums/searchpage/tab/message?filter=location&q=s21&noSynonym=false&location=forum-board:GalaxyS21&sort_by=-topicPostDate&collapse_discussion=true\"\n##html = urlopen(url, context=ctx).read()\n##soup = BeautifulSoup(html, \"html.parser\")\n##\n### Retrieve all of the post links\n##tags = soup.find_all(\"a\", class_=\"page-link lia-link-navigation lia-custom-event\")\n##print(tags)\n##lol = input()\n\n\n### (Old method) Retrieve all of the post titles & links\n##tags = soup.find_all(\"h3\", text=True)\n### print(tags)\n##\n##for eachtag in tags:\n##    # print(eachtag.text)\n##    taglist.append(eachtag.text)\n##    linklist.append(eachtag.find(\"a\")['href'])\n\n\n\n\n\n# https://stackoverflow.com/questions/23380171/using-beautifulsoup-to-extract-text-without-tags\n##### Is this more efficient? :      -> Yes!        2021.8.4.\n##threads = soup.find_all(\"article\", class_=\"samsung-message-tile samsung-thread-unread\")   # find all threads. Remove ', text=True' ! \n##print(len(threads))\n##\n##for thread in threads:\n##    lastacttime = thread.find(\"time\", datetime='true').text\n##    timewords = ['now', 'second', 'm ago', 'hour', 'yesterday']\n##\n##    # https://stackoverflow.com/questions/3271478/check-list-of-words-in-another-string\n##    if any(word in lastacttime for word in timewords):                      # if any of the keywords is found as latest comment time,\n##        taglist.append(thread.find('h3', text=True).text)                   # scrape title\n##        print(thread.find('h3', text=True).text)\n##        \n##        linklist.append(thread.find('h3', text=True).find('a')['href'])     # scrape hyperlink\n##        print(thread.find('h3', text=True).find('a')['href'])\n##        \n##        lastlist.append(lastacttime)                                        # scrape last activity time\n##        print(lastacttime)\n#####\n\n\n# Problem when trying to append sheets to existing .xlsx file\n# zipfile.BadZipFile: File is not a zip file error  ->  https://python-forum.io/thread-32833.html\n# check pandas version with: pip show pandas\n# reinstall pandas version with: pip install pandas==1.1.5\n# https://stackoverflow.com/questions/31013308/bad-zip-file-error-when-rewriting-code-in-python-3-4\n# https://stackoverflow.com/questions/28058563/write-to-stringio-object-using-pandas-excelwriter\n# https://github.com/pandas-dev/pandas/issues/28653\n\n#try:\n#    with pd.ExcelWriter(\"samsungscrap.xlsx\", mode=\"a\", engine=\"xlsxwriter\") as writer:\n#        df.to_excel(writer, sheet_name=str(date.today()))\n#except FileNotFoundError:\n#    df.to_excel(\"samsungscrap.xlsx\", sheet_name=str(date.today()))\n\n","metadata":{"tags":[],"cell_id":"05df6dbff201472a8fd01537f5075f8a","is_code_hidden":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### XDA Scraper (without Notes)","metadata":{"tags":[],"cell_id":"a188b78e74994ddeb6b6ee0b916cbbc1","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# XDA scraper\n\n# Scrape posts about Samsung's mobile devices from XDA-Developers website\n    # for new models,\n        # check whether it was posted within 1 day\n        # get post url & comment number\n        # go to next page and check again\n        # go to each url and get title, contents, # comment\n\n\nimport urllib.request\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\nimport ssl\nimport time\nimport datetime\nimport pandas as pd\n\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\n\ntaglist = []        # title\nlinklist = []       # url\nlastlist = []       # last activity time\ndatelist = []       # post creation time\nlabellist = []      # label (Y/N)\ncomlist = []        # number of comments\nmodellist = []      # device model\n\n\n###\n# For long weekends, simply add the name(s) of day that you missed(ex2), or day up to 2 days ago(ex1) to the timewords below\n# ex1. off from Fri to Tue, doing VOC on Wed --> add \"Sunday\" & \"Monday\" \n# ex2. off from Wed to Sun, doing VOC on Mon --> add \"Wed\" & \"Thur\" \n###\nvocday = input(\"Is it Monday or nah? (Enter y/n):\")\nif vocday == \"y\":\n    print(\"Gotcha fam. Take it easy.\")\n    timewords = ['now', 'second ago', 'A moment ago', '1 minute ago', 'minutes ago', 'hour ago', 'hours ago', 'Today', 'Yesterday', 'Saturday', 'Friday']\nelse:\n    print(\"Gotcha fam. One day closer to Friday, amirite?\")\n    timewords = ['now', 'second ago', 'A moment ago', '1 minute ago', 'minutes ago', 'hour ago', 'hours ago', 'Today', 'Yesterday']\n\n\ntime.sleep(2.5)\n\n\nclass AppURLopener(urllib.request.FancyURLopener):\n    version = \"Mozilla/5.0\"\nopener = AppURLopener()\n#response = opener.open('http://httpbin.org/user-agent')\n\n\n# define a function for retrieving titles & post links on a results page\ndef scraperesults(searchpage, timewords):\n    \n    print(\"Scraping a new model/page...\")\n    response = opener.open(searchpage)\n    soup = BeautifulSoup(response, \"html.parser\")\n\n    # Determine the latest activity on the posts\n    threads = soup.find_all(\"div\", class_=\n            re.compile(r\"structItem[ \\t]+structItem--thread[ \\t]+is-prefix[0-9]+[ \\t]+js-inlineModContainer[ \\t]+js-threadListItem-[0-9]+\"))   # find all threads. Remove ', text=True' ! \n    \n    for thread in threads:\n        lastacttime = thread.find(\"time\", class_=re.compile(r'structItem-latestDate[ \\t]+u-dt')).text\n        #print(lastacttime)\n\n        # https://stackoverflow.com/questions/3271478/check-list-of-words-in-another-string\n        if any(word in lastacttime for word in timewords):      # if any of the keywords is found as latest comment time,\n            taglist.append(thread.find_all('a', text=True)[1].text)             # scrape title\n            #print(thread.find_all('a', text=True)[1].text)     # find 2nd 'a' element under post\n            \n            linklist.append(\"https://forum.xda-developers.com\" + thread.find_all('a', text=True)[1]['href'])    # scrape hyperlink            \n            lastlist.append(lastacttime)                                        # scrape last activity time\n            datelist.append(thread.find_all('a', text=True)[3].text)            # scrape post creation time\n            comlist.append(int(''.join(re.findall('[0-9]+', str(thread.find('dd'))))))       # scrape number of comments\n            \n            # can't tell model by looking at final URL; strip model name from the front-page-of-the-board URL\n            modellist.append(searchpage.lstrip(\"https://forum.xda-developers.com/f/samsung-galaxy-\").rstrip(r\".[0-9]+/|.[0-9]+/page-[0-9]\").replace(\"-\",\" \"))\n            \n\n\n    print(\"-----------Going to next page...----------\")\n    time.sleep(0.5)   # add 0.5s delay\n\n\n\n# for new, major S-line models, do 1-3 pages\nsearchpages = [\"https://forum.xda-developers.com/f/samsung-galaxy-s22-ultra.12515/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-s22-ultra.12515/page-2\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-s22-ultra.12515/page-3\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-tab-s8.12521/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-fold-4.12657/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-fold-4.12657/page-2\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-flip-4.12655/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-flip-4.12655/page-2\",\n               # \"https://forum.xda-developers.com/f/samsung-galaxy-s21-fe.12389/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-s21.11937/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-fold3.12349/\",\n               \"https://forum.xda-developers.com/f/samsung-galaxy-z-flip-3.12351/\"\n               ]\n\n\nfor searchpage in searchpages:\n    scraperesults(searchpage, timewords)\n\n\nprint(\"\\nNumber of taglist: \", len(taglist), \"posts\",\n      \"\\nNumber of linklist: \", len(linklist), \"posts\")\n\n\nbodylist = []\n\n\n# Open up all links and get data (Crawl)\n\nfor hyperlink in linklist:\n    response = opener.open(hyperlink)\n    soup = BeautifulSoup(response, \"html.parser\")\n\n    print(\"Scraping...\")\n\n    try:       # for errors like \"ValueError: read of closed file\" skip with try/except. Press enter to continue\n        body = soup.find(\"div\", class_=\"bbWrapper\")\n        bodylist.append(body.text.strip())   # remove whitespace at beginning & end\n    except:\n        pass\n\n    time.sleep(0.5)      # add 0.5s delay\n\n\n\n# Create DataFrame and export to Excel file\n\ndictio = {'Model' : modellist,\n          'Priority' : [\"C\"]*len(taglist),  # create a list of C's\n          'Title' : taglist,\n          'Body Text' : bodylist,\n          'Thread' : [1]*len(taglist),   # create a list of 1's\n          'Comments' : comlist,\n          'URLs' : linklist,\n          'Start Date' : datelist,\n          'Last Date' : lastlist,\n          'Issue?' : ['Y']*len(taglist)   # as placeholder, make them all Y\n          }\ndf = pd.DataFrame(dictio)\nprint(df)\n\ninput()\n\n\ntodaystring = datetime.datetime.now().strftime(\"%Y%m%d\")    # 20220331\nfilenom = \"xdascrap\"\nfileext = \".xlsx\"\nfilename = str(filenom + todaystring + fileext)\n\ndf.to_excel(filename, sheet_name=str(datetime.date.today()))     # Create a new file (can overwrite, so careful)\n\nlol = input()\n\n\n\n# Ideas for improvement & new features\n    # Post-processing DataFrame\n    # using import csv, read table from previous file\n        # find & replace duplicates, update file\n        # for N & R, drop the new duplicate\n        # for Y, drop the old duplicate\n    # SQL database update\n\n\n","metadata":{"tags":[],"cell_id":"70d061644347403fb3544fca14812f8b","is_code_hidden":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### L","metadata":{"tags":[],"cell_id":"a3ababc214c14b5cbb45946eee10cf71","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"706b48a7bcf346f09108f9afba35dfb4","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e5caba9f-cd36-4d50-aaa3-2cf59957a2f4' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"4088f254457d4eec9b02441a587f7b0a","deepnote_execution_queue":[]}}